{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NWPUMayDayMcm2018\n",
    "# 2018西工大五一数模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 相关资源\n",
    "1. 赛题 <http://lxy.nwpu.edu.cn/info/1353/12029.htm>![baidu](../BackupSource/Pics/baidu.jpg)\n",
    "2. 本渣重构的代码&文档 <https://github.com/Alex-Beng/NwpuMayDayMcm2018>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 赛题\n",
    "\n",
    "1. 通过利用120章回中主要人物名称出现的频率的不同，能否证明不同章回之间作者的异同；\n",
    "2. 通过利用120章回中你感兴趣词语的词频（比如虚词或者常用高频词的词频）的不同，能否证明不同章回之间作者的异同；\n",
    "3. 通过对词与词之间的相关性进行分析，能否证明不同章回之间作者的异同；\n",
    "4. 除了上述三种方法以外，你是否有其他方法（比如语义分析等）来分析不同章回之间作者的异同？请建立模型并说明理由。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 分析\n",
    "0. 显然是NLP问题 or 用数学建模的角度：数据分析的题\n",
    "1. 主要人名→需要分词，并以人名词频为特征进行分类\n",
    "2. 感兴趣词→分词，并以感兴趣词为特征进行分类\n",
    "3. 词与词相关性→向量相似度？\n",
    "4. 语义→word2vec & doc2vec\n",
    "5. 将各个问题都需要用到的功能封装成sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 代码结构\n",
    "![code struture](../BackupSource/Pics/代码结构.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IO相关\n",
    "## 文件可参考 项目路径/Code/sdk/IO.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class IO:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def ReadFile(self, file_path):\n",
    "        with open(file_path, mode='r', encoding='utf-8') as f:\n",
    "            return f.readlines()\n",
    "\n",
    "    def ReadFiles(self, file_path, perfix, min_idx, max_idx):\n",
    "        file_list = []\n",
    "        for i in range(min_idx, max_idx+1):\n",
    "            file_list.append(self.ReadFile(('%s/%s-%d')%(file_path, perfix, i)))\n",
    "        return file_list\n",
    "\n",
    "    def WriteFile(self, data, file_path):\n",
    "        with open(file_path, mode='w', encoding='utf-8') as f:\n",
    "            f.write(data)\n",
    "        \n",
    "    def WriteFiles(self, data_list, file_path, perfix, min_idx, max_idx):\n",
    "        for i, j in zip(range(min_idx, max_idx+1), data_list):\n",
    "            self.WriteFile(j, \"%s/%s-%d\"%(file_path, perfix, i))\n",
    "\n",
    "    def SaveArray(self, arrays, save_path):\n",
    "        np.save(save_path, arrays)\n",
    "\n",
    "    def ReadArray(self, save_path):\n",
    "        return np.load(save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 文本预处理\n",
    "## 文件可参考 项目路径/Code/sdk/TextProc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "class TextProc():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def Divide2Chapter(self, raw_novel):\n",
    "        # 通过正则匹配 第xx回 ，然后通过匹配结果的分段\n",
    "        # 因为 第一回 之前的那段是标题作者etc，与正文无关，故删去\n",
    "        divided_parts = re.split('第.*.回 ', raw_novel)\n",
    "        del divided_parts[0]\n",
    "\n",
    "        return divided_parts\n",
    "    def RmPuntuation(self, raw_text):\n",
    "        # 通过正则匹配中文&空白符，然后取反^即是标点符号\n",
    "        # 然后使用 sub 函数替换成 ' '\n",
    "        regex = re.compile(\"[^\\u4e00-\\u9fa5a-zA-Z0-9\\s]\")\n",
    "        return regex.sub(' ', raw_text)\n",
    "    def RmEnter(self, raw_text):\n",
    "        # 回车类似删去标点\n",
    "        # 须注意 Unix/Linix 中换行 \\n 回车 \\r\n",
    "        regex = re.compile(\"[\\r\\n]\")\n",
    "        return regex.sub(' ', raw_text)\n",
    "    def Divide2Word(self, raw_text):\n",
    "        # 使用 jieba 库分词，关闭全模式\n",
    "        divided_list = jieba.cut(raw_text, cut_all=False)\n",
    "        return ' '.join(divided_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 特征工程\n",
    "## 文件可参考 项目路径/Code/sdk/Feature.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "import gensim\n",
    "\n",
    "class FeatureDraw:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def CountWord(self, divided_text, chapter_idx):\n",
    "        # 以词为 key ，频数为 value\n",
    "        # 然后根据频数对 key 进行排序后返回\n",
    "        word_freq = {}\n",
    "        word_list = divided_text.split()\n",
    "\n",
    "        for word in word_list:\n",
    "            if word in word_freq:\n",
    "                word_freq[word] += 1\n",
    "            else:\n",
    "                word_freq[word] = 1\n",
    "        return sorted(word_freq.items(), key=lambda k:k[1], reverse=True)\n",
    "    \n",
    "    def Word2Vec(self, sentences, save_path):\n",
    "        model = gensim.models.Word2Vec(sentences)\n",
    "        model.save(save_path)\n",
    "\n",
    "    def Doc2Vec(self, sentences, save_path, file_handler):\n",
    "        sentences=gensim.models.doc2vec.TaggedLineDocument(file_handler)\n",
    "        model = gensim.models.Doc2Vec(sentences)\n",
    "        model.save(save_path)\n",
    "\n",
    "class DataProcessing:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def DivideDataset(self, data_list, testset_ratio = 0.25):\n",
    "        # random 划分测试集，剩下为训练集合\n",
    "        testset_size = len(data_list)*testset_ratio\n",
    "        \n",
    "        testset = set()\n",
    "        trainset = set()\n",
    "\n",
    "        while len(testset) < testset_size:\n",
    "            lucky_dog = random.choice(data_list)\n",
    "            testset.add(lucky_dog)\n",
    "        \n",
    "        trainset = set(data_list) - testset\n",
    "\n",
    "        return testset, trainset\n",
    "    def ZeroCenterd(self, raw_vecs):\n",
    "        raw_vecs -= np.mean(raw_vecs, axis=0)\n",
    "        return raw_vecs\n",
    "    def Normalized(self, raw_vecs):\n",
    "        raw_vecs /= np.std(raw_vecs, axis=0)\n",
    "        return raw_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 分类分析\n",
    "## 文件可参考 项目路径/Code/sdk/Classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, train_data, labels):\n",
    "        self.train_data, self.test_data, self.train_label, self.test_label = train_test_split(train_data, labels, train_size=0.7, random_state=1)\n",
    "        # self.train_data = scale(self.train_data)\n",
    "        # self.test_data = scale(self.test_data)\n",
    "        \n",
    "        print(len(self.test_label))\n",
    "        print(len(self.train_label))\n",
    "        print(self.train_data.shape)\n",
    "        \n",
    "    def Train(self, save_path):\n",
    "        # self.classifier = KMeans(n_clusters=6)\n",
    "        # self.classifier = AdaBoostClassifier()\n",
    "        # self.classifier = MLPClassifier(hidden_layer_sizes=(100, 100, 100)) \n",
    "        # self.classifier = GradientBoostingClassifier()\n",
    "        # self.classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "        # self.classifier = RandomForestClassifier()\n",
    "        # self.classifier = DecisionTreeClassifier()\n",
    "        self.classifier = SVC(kernel='linear', decision_function_shape='ovr')\n",
    "        # self.classifier = LinearSVC()\n",
    "\n",
    "        self.classifier.fit(self.train_data, self.train_label)\n",
    "        # self.classifier.fit(self.train_data)\n",
    "\n",
    "        f = open(save_path, 'wb')\n",
    "        pickle.dump(self.classifier, f)\n",
    "        f.close()\n",
    "    def GetAccuracy(self):\n",
    "        print(self.classifier.score(self.train_data, self.train_label))\n",
    "        print(self.classifier.score(self.test_data, self.test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 聚类分析\n",
    "## 文件可参考 项目路径/Code/sdk/Cluster.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self, scalars):\n",
    "        self.scalars = scalars \n",
    "    def Cluster(self):\n",
    "        kmeans_cluster = KMeans(n_clusters=6)\n",
    "        kmeans_cluster.fit(self.scalars)\n",
    "        self.labels = kmeans_cluster.labels_\n",
    "    def ToPics(self):\n",
    "        pca = PCA(n_components=2)\n",
    "        two_dim_scalars = pca.fit_transform(self.scalars)\n",
    "        for i in range(len(two_dim_scalars)):\n",
    "            if i <=79:\n",
    "                if self.labels[i] == 0:\n",
    "                    plt.scatter(two_dim_scalars[i][0], two_dim_scalars[i][1], marker='x', c='yellow')\n",
    "                else:\n",
    "                    plt.scatter(two_dim_scalars[i][0], two_dim_scalars[i][1], marker='x', c='red')\n",
    "            else:\n",
    "                if self.labels[i] == 0:\n",
    "                    plt.scatter(two_dim_scalars[i][0], two_dim_scalars[i][1], marker='o', c='yellow')\n",
    "                else:\n",
    "                    plt.scatter(two_dim_scalars[i][0], two_dim_scalars[i][1], marker='o', c='red')\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 数据展示\n",
    "## 文件可参考 项目路径/Code/sdk/DataPresentation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DataPresentation:\n",
    "    def __init__(self, scalars):\n",
    "        self.scalars = scalars\n",
    "    def LowDim(self, n_dim):\n",
    "        pca=PCA(n_components=n_dim)\n",
    "        return pca.fit_transform(self.scalars)\n",
    "\n",
    "    def VecPic(self):\n",
    "        pca=PCA(n_components=2)\n",
    "        scalars = pca.fit_transform(self.scalars)\n",
    "\n",
    "        former80=scalars[0:79,:]\n",
    "        latter40=scalars[80:119,:]\n",
    "        plt.scatter(former80[:,0], former80[:,1], marker='x')\n",
    "        plt.scatter(latter40[:,0], latter40[:,1], marker='o')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 终于可以开始做问题了！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 文件可参考 项目路径/Code/Solutions/One.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 接下来...\n",
    "# ppt莫得做完...请代码发言八...\n",
    "# 主讲已经佛了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
